{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Conclusion\n",
    "\n",
    "This notebook demonstrates a complete LoRA (Low-Rank Adaptation) fine-tuning pipeline using MLX on Apple Silicon. Here's what we accomplished:\n",
    "\n",
    "### ðŸ”§ **What We Built:**\n",
    "- **LoRA Linear Layer**: Custom implementation with low-rank decomposition (A, B matrices)\n",
    "- **LoRA Transformer**: Complete transformer with LoRA-adapted attention and feed-forward layers  \n",
    "- **Synthetic Dataset**: Arithmetic, Fibonacci, and pattern sequences for training\n",
    "- **Training Pipeline**: Efficient fine-tuning loop optimizing only LoRA parameters\n",
    "- **Inference Engine**: Autoregressive generation and pattern completion\n",
    "\n",
    "### ðŸ“ˆ **Key Results:**\n",
    "- **Parameter Efficiency**: Only ~10-20% of parameters are trainable with LoRA\n",
    "- **Fast Training**: Reduced memory usage and faster convergence\n",
    "- **Pattern Learning**: Model successfully learns to complete arithmetic and Fibonacci sequences\n",
    "- **Preserved Weights**: Original model weights remain frozen and unchanged\n",
    "\n",
    "### ðŸš€ **MLX Advantages:**\n",
    "- **Apple Silicon Optimized**: Leverages Metal Performance Shaders\n",
    "- **Memory Efficient**: Automatic memory management and lazy evaluation\n",
    "- **Python-First**: Clean, PyTorch-like API with NumPy compatibility\n",
    "- **Fast Prototyping**: Rapid iteration for ML research and experimentation\n",
    "\n",
    "### ðŸ’¡ **Real-World Applications:**\n",
    "- **Domain Adaptation**: Fine-tune pre-trained models for specific tasks\n",
    "- **Multi-Task Learning**: Swap LoRA adapters for different tasks\n",
    "- **Personalization**: Create user-specific model adaptations\n",
    "- **Resource-Constrained Fine-tuning**: Efficient training on mobile/edge devices\n",
    "\n",
    "This pipeline can be extended to work with larger models (LLaMA, GPT) and real datasets by scaling the architecture and data generation components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T18:57:20.771345Z",
     "start_time": "2025-08-04T18:57:20.661214Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m fig, (ax1, ax2) = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Plot training loss\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m steps = \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrainer\u001b[49m.train_losses) + \u001b[32m1\u001b[39m)\n\u001b[32m      9\u001b[39m ax1.plot(steps, trainer.train_losses, \u001b[33m'\u001b[39m\u001b[33mb-\u001b[39m\u001b[33m'\u001b[39m, alpha=\u001b[32m0.7\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mTraining Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Add smoothed line\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot training loss\n",
    "steps = range(1, len(trainer.train_losses) + 1)\n",
    "ax1.plot(steps, trainer.train_losses, 'b-', alpha=0.7, label='Training Loss')\n",
    "\n",
    "# Add smoothed line\n",
    "window_size = 10\n",
    "if len(trainer.train_losses) > window_size:\n",
    "    smoothed_loss = np.convolve(trainer.train_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "    smooth_steps = range(window_size, len(trainer.train_losses) + 1)\n",
    "    ax1.plot(smooth_steps, smoothed_loss, 'r-', linewidth=2, label=f'Smoothed (window={window_size})')\n",
    "\n",
    "ax1.set_xlabel('Training Steps')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('LoRA Fine-tuning: Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training accuracy\n",
    "ax2.plot(steps, trainer.train_accuracies, 'g-', alpha=0.7, label='Training Accuracy')\n",
    "\n",
    "# Add smoothed line\n",
    "if len(trainer.train_accuracies) > window_size:\n",
    "    smoothed_acc = np.convolve(trainer.train_accuracies, np.ones(window_size)/window_size, mode='valid')\n",
    "    ax2.plot(smooth_steps, smoothed_acc, 'orange', linewidth=2, label=f'Smoothed (window={window_size})')\n",
    "\n",
    "ax2.set_xlabel('Training Steps')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('LoRA Fine-tuning: Training Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print training statistics\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ LORA FINE-TUNING RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"ðŸ“Š Training Statistics:\")\n",
    "print(f\"   â€¢ Total training steps: {len(trainer.train_losses)}\")\n",
    "print(f\"   â€¢ Initial loss: {trainer.train_losses[0]:.4f}\")\n",
    "print(f\"   â€¢ Final loss: {trainer.train_losses[-1]:.4f}\")\n",
    "print(f\"   â€¢ Loss reduction: {(trainer.train_losses[0] - trainer.train_losses[-1]):.4f}\")\n",
    "print(f\"   â€¢ Initial accuracy: {trainer.train_accuracies[0]:.4f}\")\n",
    "print(f\"   â€¢ Final accuracy: {trainer.train_accuracies[-1]:.4f}\")\n",
    "print(f\"   â€¢ Best accuracy: {max(trainer.train_accuracies):.4f}\")\n",
    "\n",
    "print(f\"\\\\nðŸ”§ Model Architecture:\")\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"   â€¢ Total parameters: {total_params:,}\")\n",
    "print(f\"   â€¢ Trainable (LoRA) parameters: {trainable_params:,}\")\n",
    "print(f\"   â€¢ Parameter efficiency: {trainable_params/total_params*100:.2f}% trainable\")\n",
    "print(f\"   â€¢ LoRA rank: {model.blocks[0].attention.q_proj.rank}\")\n",
    "print(f\"   â€¢ LoRA alpha: {model.blocks[0].attention.q_proj.alpha}\")\n",
    "\n",
    "print(f\"\\\\nðŸ§® Test Performance:\")\n",
    "print(f\"   â€¢ Test loss: {test_loss:.4f}\")\n",
    "print(f\"   â€¢ Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\\\nðŸ’¡ Key Benefits of LoRA:\")\n",
    "print(f\"   â€¢ Dramatically reduced trainable parameters ({trainable_params:,} vs {total_params:,})\")\n",
    "print(f\"   â€¢ Fast fine-tuning with minimal memory overhead\")\n",
    "print(f\"   â€¢ Original model weights preserved (can be easily swapped)\")\n",
    "print(f\"   â€¢ Task-specific adaptations without catastrophic forgetting\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"âœ… LoRA Fine-tuning Pipeline Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Progress Visualization\n",
    "\n",
    "Let's visualize the training progress to see how the LoRA fine-tuning improved the model's performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAInference:\n",
    "    \\\"\\\"\\\"Inference utilities for the LoRA fine-tuned model\\\"\\\"\\\"\n",
    "    \n",
    "    def __init__(self, model, data_generator):\n",
    "        self.model = model\n",
    "        self.data_gen = data_generator\n",
    "    \n",
    "    def generate_sequence(self, prompt_tokens: List[int], max_length: int = 20, temperature: float = 0.8):\n",
    "        \\\"\\\"\\\"Generate a sequence using autoregressive sampling\\\"\\\"\\\"\n",
    "        generated = prompt_tokens.copy()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Prepare input (pad to model's expected length)\n",
    "            input_seq = generated + [self.data_gen.pad_token] * (self.data_gen.seq_len - len(generated))\n",
    "            input_seq = input_seq[:self.data_gen.seq_len]  # Truncate if too long\n",
    "            \n",
    "            # Get model predictions\n",
    "            input_tensor = mx.array([input_seq])\n",
    "            with mx.no_grad():\n",
    "                logits = self.model(input_tensor)\n",
    "            \n",
    "            # Get logits for the last generated position\n",
    "            next_token_logits = logits[0, len(generated) - 1, :] / temperature\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = mx.softmax(next_token_logits, axis=-1)\n",
    "            next_token = mx.random.categorical(probs.reshape(1, -1)).item()\n",
    "            \n",
    "            # Stop if we generate end token or pad token\n",
    "            if next_token == self.data_gen.end_token or next_token == self.data_gen.pad_token:\n",
    "                break\n",
    "            \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            # Stop if sequence gets too long\n",
    "            if len(generated) >= self.data_gen.seq_len - 1:\n",
    "                break\n",
    "        \n",
    "        return generated\n",
    "    \n",
    "    def evaluate_on_test_set(self, num_examples: int = 50):\n",
    "        \\\"\\\"\\\"Evaluate model on a test set\\\"\\\"\\\"\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        \n",
    "        for _ in range(num_examples):\n",
    "            input_batch, target_batch = self.data_gen.generate_batch(1)\n",
    "            \n",
    "            with mx.no_grad():\n",
    "                logits = self.model(input_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = trainer.compute_loss(logits, target_batch)\n",
    "                total_loss += float(loss)\n",
    "                \n",
    "                # Compute accuracy\n",
    "                accuracy = trainer.compute_accuracy(logits, target_batch)\n",
    "                total_accuracy += float(accuracy)\n",
    "        \n",
    "        return total_loss / num_examples, total_accuracy / num_examples\n",
    "    \n",
    "    def demonstrate_pattern_completion(self):\n",
    "        \\\"\\\"\\\"Demonstrate the model's ability to complete patterns\\\"\\\"\\\"\n",
    "        print(\"Pattern Completion Demonstrations:\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Test arithmetic sequence\n",
    "        print(\"\\\\n1. Arithmetic Sequence (2, 4, 6, 8, ...):\")\n",
    "        arithmetic_prompt = [self.data_gen.start_token, \n",
    "                            self.data_gen.num_start + 2, \n",
    "                            self.data_gen.num_start + 4, \n",
    "                            self.data_gen.num_start + 6]\n",
    "        generated = self.generate_sequence(arithmetic_prompt, max_length=10)\n",
    "        decoded = decode_sequence(generated, self.data_gen)\n",
    "        print(f\"Prompt: {' '.join(decoded[:4])}\")\n",
    "        print(f\"Generated: {' '.join(decoded)}\")\n",
    "        \n",
    "        # Test Fibonacci-like sequence\n",
    "        print(\"\\\\n2. Fibonacci-like Sequence (1, 1, 2, 3, ...):\")\n",
    "        fib_prompt = [self.data_gen.start_token,\n",
    "                     self.data_gen.num_start + 1,\n",
    "                     self.data_gen.num_start + 1,\n",
    "                     self.data_gen.num_start + 2]\n",
    "        generated = self.generate_sequence(fib_prompt, max_length=10)\n",
    "        decoded = decode_sequence(generated, self.data_gen)\n",
    "        print(f\"Prompt: {' '.join(decoded[:4])}\")\n",
    "        print(f\"Generated: {' '.join(decoded)}\")\n",
    "        \n",
    "        # Test pattern sequence\n",
    "        print(\"\\\\n3. Pattern Sequence (1, 2, 3, 1, 2, 3, ...):\")\n",
    "        pattern_prompt = [self.data_gen.start_token,\n",
    "                         self.data_gen.num_start + 1,\n",
    "                         self.data_gen.num_start + 2,\n",
    "                         self.data_gen.num_start + 3,\n",
    "                         self.data_gen.num_start + 1]\n",
    "        generated = self.generate_sequence(pattern_prompt, max_length=12)\n",
    "        decoded = decode_sequence(generated, self.data_gen)\n",
    "        print(f\"Prompt: {' '.join(decoded[:5])}\")\n",
    "        print(f\"Generated: {' '.join(decoded)}\")\n",
    "\n",
    "# Create inference object and run evaluations\n",
    "print(\"Setting up inference and evaluation...\")\n",
    "inference = LoRAInference(model, data_gen)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\\\nEvaluating on test set...\")\n",
    "test_loss, test_accuracy = inference.evaluate_on_test_set(num_examples=100)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Demonstrate pattern completion\n",
    "print(\"\\\\n\")\n",
    "inference.demonstrate_pattern_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Inference\n",
    "\n",
    "Let's test our fine-tuned LoRA model by evaluating it on some test sequences and demonstrating autoregressive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"ðŸš€ Starting LoRA Fine-tuning Training!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train for 200 steps\n",
    "trainer.train(num_steps=200, batch_size=8, log_interval=25)\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"Training Summary:\")\n",
    "print(f\"Final loss: {trainer.train_losses[-1]:.4f}\")\n",
    "print(f\"Final accuracy: {trainer.train_accuracies[-1]:.4f}\")\n",
    "print(f\"Best accuracy: {max(trainer.train_accuracies):.4f}\")\n",
    "print(f\"Loss improvement: {trainer.train_losses[0]:.4f} â†’ {trainer.train_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LoRA Fine-tuning Training\n",
    "\n",
    "Let's train our model with LoRA adapters for a few hundred steps to see it learn the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRATrainer:\n",
    "    \"\"\"Trainer class for LoRA fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, model, data_generator, learning_rate: float = 1e-3):\n",
    "        self.model = model\n",
    "        self.data_generator = data_generator\n",
    "        \n",
    "        # Only optimize LoRA parameters and embeddings\n",
    "        self.trainable_params = self.get_trainable_parameters()\n",
    "        self.optimizer = optim.Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        # Training metrics\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"Get only the trainable parameters (LoRA params + embeddings)\"\"\"\n",
    "        trainable = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not getattr(param, 'stop_gradient', False):\n",
    "                trainable[name] = param\n",
    "        return trainable\n",
    "    \n",
    "    def compute_loss(self, logits, targets, ignore_index=0):\n",
    "        \"\"\"Compute cross-entropy loss, ignoring padding tokens\"\"\"\n",
    "        # Reshape for loss computation\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_flat = logits.reshape(-1, vocab_size)\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        \n",
    "        # Create mask to ignore padding tokens\n",
    "        mask = targets_flat != ignore_index\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        log_probs = mx.log_softmax(logits_flat, axis=-1)\n",
    "        loss = -mx.take_along_axis(log_probs, targets_flat.reshape(-1, 1), axis=1).squeeze(1)\n",
    "        \n",
    "        # Apply mask and compute mean\n",
    "        masked_loss = loss * mask\n",
    "        return mx.sum(masked_loss) / mx.sum(mask)\n",
    "    \n",
    "    def compute_accuracy(self, logits, targets, ignore_index=0):\n",
    "        \"\"\"Compute token-level accuracy, ignoring padding tokens\"\"\"\n",
    "        predictions = mx.argmax(logits, axis=-1)\n",
    "        mask = targets != ignore_index\n",
    "        correct = (predictions == targets) * mask\n",
    "        return mx.sum(correct).astype(mx.float32) / mx.sum(mask).astype(mx.float32)\n",
    "    \n",
    "    def train_step(self, input_batch, target_batch):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        def loss_fn():\n",
    "            logits = self.model(input_batch)\n",
    "            loss = self.compute_loss(logits, target_batch)\n",
    "            return loss\n",
    "        \n",
    "        # Forward pass and compute gradients\n",
    "        loss, grads = mx.value_and_grad(loss_fn)()\n",
    "        \n",
    "        # Update only trainable parameters\n",
    "        self.optimizer.update(self.model, grads)\n",
    "        mx.eval(self.model.parameters())\n",
    "        \n",
    "        # Compute accuracy\n",
    "        with mx.no_grad():\n",
    "            logits = self.model(input_batch)\n",
    "            accuracy = self.compute_accuracy(logits, target_batch)\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def train(self, num_steps: int, batch_size: int = 8, log_interval: int = 50):\n",
    "        \"\"\"Train the model for specified number of steps\"\"\"\n",
    "        print(f\"Starting LoRA fine-tuning for {num_steps} steps...\")\n",
    "        print(f\"Trainable parameters: {len(self.trainable_params)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # Generate batch\n",
    "            input_batch, target_batch = self.data_generator.generate_batch(batch_size)\n",
    "            \n",
    "            # Training step\n",
    "            loss, accuracy = self.train_step(input_batch, target_batch)\n",
    "            \n",
    "            # Store metrics\n",
    "            self.train_losses.append(float(loss))\n",
    "            self.train_accuracies.append(float(accuracy))\n",
    "            \n",
    "            # Logging\n",
    "            if (step + 1) % log_interval == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_loss = np.mean(self.train_losses[-log_interval:])\n",
    "                avg_acc = np.mean(self.train_accuracies[-log_interval:])\n",
    "                \n",
    "                print(f\"Step {step + 1:4d} | \"\n",
    "                      f\"Loss: {avg_loss:.4f} | \"\n",
    "                      f\"Accuracy: {avg_acc:.4f} | \"\n",
    "                      f\"Time: {elapsed_time:.1f}s\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "\n",
    "# Initialize model and trainer\n",
    "print(\"Initializing LoRA Trainer...\")\n",
    "vocab_size = 1000\n",
    "model = SimpleLoRATransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    d_ff=512,\n",
    "    lora_rank=8,\n",
    "    lora_alpha=16.0\n",
    ")\n",
    "\n",
    "data_gen = SyntheticDataGenerator(vocab_size=vocab_size, seq_len=32)\n",
    "trainer = LoRATrainer(model, data_gen, learning_rate=1e-3)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.size for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {len(trainer.trainable_params)}\")\n",
    "\n",
    "# Show which parameters are trainable\n",
    "print(\"\\\\nTrainable parameters:\")\n",
    "for name, param in trainer.trainable_params.items():\n",
    "    print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "# Test a single training step\n",
    "print(\"\\\\nTesting single training step...\")\n",
    "input_batch, target_batch = data_gen.generate_batch(4)\n",
    "initial_loss, initial_acc = trainer.train_step(input_batch, target_batch)\n",
    "print(f\"Initial loss: {initial_loss:.4f}\")\n",
    "print(f\"Initial accuracy: {initial_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with LoRA Fine-tuning\n",
    "\n",
    "Now let's implement the training loop that will fine-tune our model using LoRA adapters. We'll use cross-entropy loss and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataGenerator:\n",
    "    \"\"\"Generate synthetic sequences for language modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 1000, seq_len: int = 32):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = 0\n",
    "        self.start_token = 1\n",
    "        self.end_token = 2\n",
    "        self.sep_token = 3\n",
    "        \n",
    "        # Reserve some tokens for numbers and operations\n",
    "        self.num_start = 10  # Numbers start from token 10\n",
    "        self.max_num = 100   # Support numbers up to 100\n",
    "        \n",
    "    def generate_arithmetic_sequence(self, start: int, step: int, length: int) -> List[int]:\n",
    "        \"\"\"Generate an arithmetic sequence\"\"\"\n",
    "        sequence = []\n",
    "        for i in range(length):\n",
    "            num = start + i * step\n",
    "            if num < self.max_num:\n",
    "                sequence.append(self.num_start + num)\n",
    "            else:\n",
    "                break\n",
    "        return sequence\n",
    "    \n",
    "    def generate_fibonacci_sequence(self, length: int) -> List[int]:\n",
    "        \"\"\"Generate a Fibonacci sequence\"\"\"\n",
    "        if length == 0:\n",
    "            return []\n",
    "        elif length == 1:\n",
    "            return [self.num_start + 1]\n",
    "        \n",
    "        sequence = [self.num_start + 1, self.num_start + 1]  # F(1)=1, F(2)=1\n",
    "        \n",
    "        for i in range(2, length):\n",
    "            next_fib = (sequence[-1] - self.num_start) + (sequence[-2] - self.num_start)\n",
    "            if next_fib < self.max_num:\n",
    "                sequence.append(self.num_start + next_fib)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def generate_pattern_sequence(self, pattern: List[int], repeats: int) -> List[int]:\n",
    "        \"\"\"Generate a repeating pattern\"\"\"\n",
    "        sequence = []\n",
    "        for _ in range(repeats):\n",
    "            sequence.extend([self.num_start + x for x in pattern])\n",
    "        return sequence\n",
    "    \n",
    "    def create_training_example(self) -> Tuple[List[int], List[int]]:\n",
    "        \"\"\"Create a single training example with input and target\"\"\"\n",
    "        sequence_type = random.choice(['arithmetic', 'fibonacci', 'pattern'])\n",
    "        \n",
    "        if sequence_type == 'arithmetic':\n",
    "            start = random.randint(1, 10)\n",
    "            step = random.randint(1, 5)\n",
    "            length = random.randint(5, 15)\n",
    "            sequence = self.generate_arithmetic_sequence(start, step, length)\n",
    "        \n",
    "        elif sequence_type == 'fibonacci':\n",
    "            length = random.randint(5, 12)\n",
    "            sequence = self.generate_fibonacci_sequence(length)\n",
    "        \n",
    "        else:  # pattern\n",
    "            pattern_length = random.randint(2, 4)\n",
    "            pattern = [random.randint(1, 10) for _ in range(pattern_length)]\n",
    "            repeats = random.randint(2, 5)\n",
    "            sequence = self.generate_pattern_sequence(pattern, repeats)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(sequence) > self.seq_len - 3:  # Leave room for special tokens\n",
    "            sequence = sequence[:self.seq_len - 3]\n",
    "        \n",
    "        # Create input and target\n",
    "        # Input: [START] + sequence[:-1]\n",
    "        # Target: sequence[:-1] + [END]\n",
    "        input_seq = [self.start_token] + sequence[:-1]\n",
    "        target_seq = sequence[:-1] + [self.end_token]\n",
    "        \n",
    "        # Pad sequences\n",
    "        while len(input_seq) < self.seq_len:\n",
    "            input_seq.append(self.pad_token)\n",
    "        while len(target_seq) < self.seq_len:\n",
    "            target_seq.append(self.pad_token)\n",
    "        \n",
    "        return input_seq[:self.seq_len], target_seq[:self.seq_len]\n",
    "    \n",
    "    def generate_batch(self, batch_size: int) -> Tuple[mx.array, mx.array]:\n",
    "        \"\"\"Generate a batch of training examples\"\"\"\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            input_seq, target_seq = self.create_training_example()\n",
    "            inputs.append(input_seq)\n",
    "            targets.append(target_seq)\n",
    "        \n",
    "        return mx.array(inputs), mx.array(targets)\n",
    "\n",
    "# Test the data generator\n",
    "print(\"Testing Synthetic Data Generator:\")\n",
    "data_gen = SyntheticDataGenerator(vocab_size=1000, seq_len=32)\n",
    "\n",
    "# Generate a few examples\n",
    "for i in range(3):\n",
    "    input_seq, target_seq = data_gen.create_training_example()\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Input:  {input_seq[:15]}...\")  # Show first 15 tokens\n",
    "    print(f\"  Target: {target_seq[:15]}...\")\n",
    "    print()\n",
    "\n",
    "# Generate a batch\n",
    "batch_size = 4\n",
    "input_batch, target_batch = data_gen.generate_batch(batch_size)\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  Inputs: {input_batch.shape}\")\n",
    "print(f\"  Targets: {target_batch.shape}\")\n",
    "\n",
    "# Show some actual sequences (decode back to numbers)\n",
    "def decode_sequence(seq, data_gen):\n",
    "    \\\"\\\"\\\"Helper function to decode sequence back to readable numbers\\\"\\\"\\\"\n",
    "    decoded = []\n",
    "    for token in seq:\n",
    "        if token == data_gen.start_token:\n",
    "            decoded.append(\"[START]\")\n",
    "        elif token == data_gen.end_token:\n",
    "            decoded.append(\"[END]\")\n",
    "        elif token == data_gen.pad_token:\n",
    "            decoded.append(\"[PAD]\")\n",
    "        elif token >= data_gen.num_start:\n",
    "            decoded.append(str(token - data_gen.num_start))\n",
    "        else:\n",
    "            decoded.append(f\"[{token}]\")\n",
    "    return decoded\n",
    "\n",
    "print(\"\\\\nDecoded example:\")\n",
    "example_input = input_batch[0].tolist()\n",
    "example_target = target_batch[0].tolist()\n",
    "print(f\"Input:  {' '.join(decode_sequence(example_input[:20], data_gen))}\")\n",
    "print(f\"Target: {' '.join(decode_sequence(example_target[:20], data_gen))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset Generation\n",
    "\n",
    "For this demonstration, we'll create a synthetic dataset that simulates a simple language modeling task. The task will be to predict arithmetic sequences and simple patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with LoRA adapters\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, lora_rank: int = 8, lora_alpha: float = 16.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # LoRA-adapted projection layers\n",
    "        self.q_proj = LoRALinear(d_model, d_model, rank=lora_rank, alpha=lora_alpha)\n",
    "        self.k_proj = LoRALinear(d_model, d_model, rank=lora_rank, alpha=lora_alpha)\n",
    "        self.v_proj = LoRALinear(d_model, d_model, rank=lora_rank, alpha=lora_alpha)\n",
    "        self.o_proj = LoRALinear(d_model, d_model, rank=lora_rank, alpha=lora_alpha)\n",
    "        \n",
    "        self.scale = 1.0 / (self.d_k ** 0.5)\n",
    "    \n",
    "    def __call__(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        k = self.k_proj(x).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        v = self.v_proj(x).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = (q @ k.transpose(0, 1, 3, 2)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores + mask\n",
    "        \n",
    "        attn_weights = mx.softmax(scores, axis=-1)\n",
    "        attn_output = attn_weights @ v\n",
    "        \n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, self.d_model)\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "\n",
    "class LoRAFeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network with LoRA adapters\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, lora_rank: int = 8, lora_alpha: float = 16.0):\n",
    "        super().__init__()\n",
    "        self.linear1 = LoRALinear(d_model, d_ff, rank=lora_rank, alpha=lora_alpha)\n",
    "        self.linear2 = LoRALinear(d_ff, d_model, rank=lora_rank, alpha=lora_alpha)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.linear2(self.activation(self.linear1(x)))\n",
    "\n",
    "\n",
    "class LoRATransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with LoRA adapters\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, lora_rank: int = 8, lora_alpha: float = 16.0):\n",
    "        super().__init__()\n",
    "        self.attention = LoRAMultiHeadAttention(d_model, num_heads, lora_rank, lora_alpha)\n",
    "        self.feed_forward = LoRAFeedForward(d_model, d_ff, lora_rank, lora_alpha)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def __call__(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.attention(self.norm1(x), mask)\n",
    "        x = x + attn_output\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(self.norm2(x))\n",
    "        x = x + ff_output\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleLoRATransformer(nn.Module):\n",
    "    \"\"\"Simple transformer model with LoRA adapters for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int = 128, num_heads: int = 8, \n",
    "                 num_layers: int = 4, d_ff: int = 512, max_seq_len: int = 128,\n",
    "                 lora_rank: int = 8, lora_alpha: float = 16.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embeddings (these will be fine-tuned normally, not with LoRA)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks with LoRA\n",
    "        self.blocks = [\n",
    "            LoRATransformerBlock(d_model, num_heads, d_ff, lora_rank, lora_alpha)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Output projection with LoRA\n",
    "        self.output_projection = LoRALinear(d_model, vocab_size, rank=lora_rank, alpha=lora_alpha)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def __call__(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Create position ids\n",
    "        position_ids = mx.arange(seq_len).reshape(1, seq_len).repeat(batch_size, axis=0)\n",
    "        \n",
    "        # Embeddings\n",
    "        token_emb = self.token_embedding(input_ids)\n",
    "        pos_emb = self.position_embedding(position_ids)\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        # Create causal mask for autoregressive generation\n",
    "        if attention_mask is None:\n",
    "            causal_mask = mx.triu(mx.ones((seq_len, seq_len)) * -1e9, k=1)\n",
    "            attention_mask = causal_mask.reshape(1, 1, seq_len, seq_len)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attention_mask)\n",
    "        \n",
    "        # Final normalization and output projection\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create and test the model\n",
    "print(\"Creating LoRA Transformer Model:\")\n",
    "vocab_size = 1000\n",
    "model = SimpleLoRATransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    d_ff=512,\n",
    "    lora_rank=8,\n",
    "    lora_alpha=16.0\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size, seq_len = 2, 16\n",
    "input_ids = mx.random.randint(0, vocab_size, (batch_size, seq_len))\n",
    "logits = model(input_ids)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    total = 0\n",
    "    trainable = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.size\n",
    "        total += param_count\n",
    "        if not getattr(param, 'stop_gradient', False):\n",
    "            trainable += param_count\n",
    "    return total, trainable\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Trainable ratio: {trainable_params / total_params * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Transformer Model with LoRA Adapters\n",
    "\n",
    "Now let's create a simple transformer model that uses LoRA adapters for fine-tuning. We'll implement attention mechanisms and feed-forward networks with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) Linear Layer\n",
    "    \n",
    "    This replaces a standard linear layer with a LoRA-adapted version.\n",
    "    The original weights are frozen, and we learn low-rank adaptation matrices.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, rank: int = 8, alpha: float = 16.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Original linear layer (frozen)\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "        # LoRA parameters\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Low-rank matrices\n",
    "        # A: (in_features, rank) - initialized with random normal\n",
    "        # B: (rank, out_features) - initialized with zeros\n",
    "        self.lora_A = mx.random.normal((in_features, rank)) * 0.01\n",
    "        self.lora_B = mx.zeros((rank, out_features))\n",
    "        \n",
    "        # Optional dropout\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else None\n",
    "        \n",
    "        # Freeze original linear layer parameters\n",
    "        self.linear.weight.stop_gradient = True\n",
    "        if hasattr(self.linear, 'bias') and self.linear.bias is not None:\n",
    "            self.linear.bias.stop_gradient = True\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Original linear transformation\n",
    "        result = self.linear(x)\n",
    "        \n",
    "        # LoRA adaptation: x @ A @ B\n",
    "        lora_x = x @ self.lora_A\n",
    "        if self.dropout is not None:\n",
    "            lora_x = self.dropout(lora_x)\n",
    "        lora_result = lora_x @ self.lora_B\n",
    "        \n",
    "        # Scale and add LoRA adaptation\n",
    "        return result + lora_result * self.scaling\n",
    "\n",
    "# Test the LoRA layer\n",
    "print(\"Testing LoRA Linear Layer:\")\n",
    "batch_size, seq_len, d_model = 4, 10, 64\n",
    "x = mx.random.normal((batch_size, seq_len, d_model))\n",
    "\n",
    "lora_layer = LoRALinear(d_model, d_model, rank=8, alpha=16.0)\n",
    "output = lora_layer(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"LoRA rank: {lora_layer.rank}\")\n",
    "print(f\"Scaling factor: {lora_layer.scaling}\")\n",
    "\n",
    "# Count trainable parameters\n",
    "original_params = lora_layer.linear.weight.size + (lora_layer.linear.bias.size if hasattr(lora_layer.linear, 'bias') and lora_layer.linear.bias is not None else 0)\n",
    "lora_params = lora_layer.lora_A.size + lora_layer.lora_B.size\n",
    "print(f\"Original parameters: {original_params} (frozen)\")\n",
    "print(f\"LoRA parameters: {lora_params} (trainable)\")\n",
    "print(f\"Parameter reduction: {lora_params / original_params * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLXTraining\n",
    "\n",
    "Welcome to the MLXTraining project! This notebook contains experiments and training code using MLX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for LoRA fine-tuning pipeline\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, List\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Add your MLX training experiments below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLXTraining LoRA Fine-tuning Pipeline initialized!\")\n",
    "print(f\"MLX version: {mx.__version__}\")\n",
    "print(f\"Running on: {mx.default_device()}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "mx.random.seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA (Low-Rank Adaptation) Implementation\n",
    "\n",
    "LoRA is a parameter-efficient fine-tuning technique that decomposes weight updates into low-rank matrices. Instead of updating all parameters, we learn two smaller matrices A and B such that Î”W = B @ A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
